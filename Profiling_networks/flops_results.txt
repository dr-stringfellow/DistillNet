GNN
Profile:
node name | # float_ops
_TFProfRoot (--/6.59b flops)
  model/Gap0/AttnFeat/conv2d/Conv2D (1.52b/1.52b flops)
  model/Gap1/AttnFeat/conv2d_4/Conv2D (1.47b/1.47b flops)
  model/dense_7/Tensordot/MatMul (589.82m/589.82m flops)
  model/tf.linalg.matmul/MatMul (324.00m/324.00m flops)
  model/dense_6/Tensordot/MatMul (294.91m/294.91m flops)
  model/dense_1/Tensordot/MatMul (147.46m/147.46m flops)
  model/dense_2/Tensordot/MatMul (147.46m/147.46m flops)
  model/dense_3/Tensordot/MatMul (147.46m/147.46m flops)
  model/dense_4/Tensordot/MatMul (147.46m/147.46m flops)
  model/dense_5/Tensordot/MatMul (147.46m/147.46m flops)
  model/tf.__operators__.add/AddV2 (81.00m/81.00m flops)
  model/tf.__operators__.add_1/AddV2 (81.00m/81.00m flops)
  model/tf.__operators__.add_2/AddV2 (81.00m/81.00m flops)
  model/tf.__operators__.add_3/AddV2 (81.00m/81.00m flops)
  model/tf.__operators__.add_4/AddV2 (81.00m/81.00m flops)
  model/tf.math.equal/Equal (81.00m/81.00m flops)
  model/tf.math.greater_equal/GreaterEqual (81.00m/81.00m flops)
  model/tf.math.multiply/Mul (81.00m/81.00m flops)
  model/tf.math.negative/Neg (81.00m/81.00m flops)
  model/tf.math.square_1/Square (81.00m/81.00m flops)
  model/tf.math.square_2/Square (81.00m/81.00m flops)
  model/tf.math.subtract/Sub (81.00m/81.00m flops)
  model/tf.math.subtract_1/Sub (81.00m/81.00m flops)
  model/tf.math.subtract_2/Sub (81.00m/81.00m flops)
  model/Gap0/AttnFeat/conv2d_3/Conv2D (73.73m/73.73m flops)
  model/Gap1/AttnFeat/conv2d_7/Conv2D (73.73m/73.73m flops)
  model/dense/Tensordot/MatMul (32.26m/32.26m flops)
  model/Gap0/AttnFeat/MatMul (23.04m/23.04m flops)
  model/Gap0/AttnFeat/conv2d_2/Conv2D (23.04m/23.04m flops)
  model/Gap0/AttnFeat/group_normalization_1/moments/SquaredDifference (23.04m/23.04m flops)
  model/Gap1/AttnFeat/MatMul (23.04m/23.04m flops)
  model/Gap1/AttnFeat/conv2d_6/Conv2D (23.04m/23.04m flops)
  model/Gap1/AttnFeat/group_normalization_3/moments/SquaredDifference (23.04m/23.04m flops)
  model/Gap0/AttnFeat/conv2d/BiasAdd (11.52m/11.52m flops)
  model/Gap0/AttnFeat/group_normalization_1/batchnorm/add_1 (11.52m/11.52m flops)
  model/Gap0/AttnFeat/group_normalization_1/batchnorm/mul_1 (11.52m/11.52m flops)
  model/Gap0/AttnFeat/group_normalization_1/moments/mean (11.52m/11.52m flops)
  model/Gap0/AttnFeat/group_normalization_1/moments/variance (11.52m/11.52m flops)
  model/Gap0/AttnFeat/sub (11.52m/11.52m flops)
  model/Gap1/AttnFeat/conv2d_4/BiasAdd (11.52m/11.52m flops)
  model/Gap1/AttnFeat/group_normalization_3/batchnorm/add_1 (11.52m/11.52m flops)
  model/Gap1/AttnFeat/group_normalization_3/batchnorm/mul_1 (11.52m/11.52m flops)
  model/Gap1/AttnFeat/group_normalization_3/moments/mean (11.52m/11.52m flops)
  model/Gap1/AttnFeat/group_normalization_3/moments/variance (11.52m/11.52m flops)
  model/Gap1/AttnFeat/sub (11.52m/11.52m flops)
  model/dense_6/BiasAdd (2.30m/2.30m flops)
  model/dense_8/Tensordot/MatMul (2.30m/2.30m flops)
  model/Gap0/AttnFeat/conv2d_1/Conv2D (1.15m/1.15m flops)
  model/Gap0/AttnFeat/group_normalization/moments/SquaredDifference (1.15m/1.15m flops)
  model/Gap1/AttnFeat/conv2d_5/Conv2D (1.15m/1.15m flops)
  model/Gap1/AttnFeat/group_normalization_2/moments/SquaredDifference (1.15m/1.15m flops)
  model/dense/BiasAdd (1.15m/1.15m flops)
  model/dense_2/BiasAdd (1.15m/1.15m flops)
  model/dense_2/Gelu/add (1.15m/1.15m flops)
  model/dense_2/Gelu/mul (1.15m/1.15m flops)
  model/dense_2/Gelu/mul_1 (1.15m/1.15m flops)
  model/dense_2/Gelu/truediv (1.15m/1.15m flops)
  model/dense_4/BiasAdd (1.15m/1.15m flops)
  model/dense_4/Gelu/add (1.15m/1.15m flops)
  model/dense_4/Gelu/mul (1.15m/1.15m flops)
  model/dense_4/Gelu/mul_1 (1.15m/1.15m flops)
  model/dense_4/Gelu/truediv (1.15m/1.15m flops)
  model/dense_7/BiasAdd (1.15m/1.15m flops)
  model/layer_normalization/moments/SquaredDifference (1.15m/1.15m flops)
  model/layer_normalization_1/moments/SquaredDifference (1.15m/1.15m flops)
  model/layer_normalization_2/moments/SquaredDifference (1.15m/1.15m flops)
  model/Gap0/AttnFeat/Softmax (900.00k/900.00k flops)
  model/Gap1/AttnFeat/Softmax (900.00k/900.00k flops)
  model/Gap0/AttnFeat/group_normalization/batchnorm/add_1 (576.00k/576.00k flops)
  model/Gap0/AttnFeat/group_normalization/batchnorm/mul_1 (576.00k/576.00k flops)
  model/Gap0/AttnFeat/group_normalization/moments/mean (576.00k/576.00k flops)
  model/Gap0/AttnFeat/group_normalization/moments/variance (576.00k/576.00k flops)
  model/Gap0/Mean (576.00k/576.00k flops)
  model/Gap1/AttnFeat/group_normalization_2/batchnorm/add_1 (576.00k/576.00k flops)
  model/Gap1/AttnFeat/group_normalization_2/batchnorm/mul_1 (576.00k/576.00k flops)
  model/Gap1/AttnFeat/group_normalization_2/moments/mean (576.00k/576.00k flops)
  model/Gap1/AttnFeat/group_normalization_2/moments/variance (576.00k/576.00k flops)
  model/Gap1/Mean (576.00k/576.00k flops)
  model/dense_1/BiasAdd (576.00k/576.00k flops)
  model/dense_3/BiasAdd (576.00k/576.00k flops)
  model/dense_3/Gelu/add (576.00k/576.00k flops)
  model/dense_3/Gelu/mul (576.00k/576.00k flops)
  model/dense_3/Gelu/mul_1 (576.00k/576.00k flops)
  model/dense_3/Gelu/truediv (576.00k/576.00k flops)
  model/dense_5/BiasAdd (576.00k/576.00k flops)
  model/dense_5/Gelu/add (576.00k/576.00k flops)
  model/dense_5/Gelu/mul (576.00k/576.00k flops)
  model/dense_5/Gelu/mul_1 (576.00k/576.00k flops)
  model/dense_5/Gelu/truediv (576.00k/576.00k flops)
  model/layer_normalization/batchnorm/add_1 (576.00k/576.00k flops)
  model/layer_normalization/batchnorm/mul (576.00k/576.00k flops)
  model/layer_normalization/batchnorm/mul_1 (576.00k/576.00k flops)
  model/layer_normalization/batchnorm/mul_2 (576.00k/576.00k flops)
  model/layer_normalization/batchnorm/sub (576.00k/576.00k flops)
  model/layer_normalization/moments/mean (576.00k/576.00k flops)
  model/layer_normalization/moments/variance (576.00k/576.00k flops)
  model/layer_normalization_1/batchnorm/add_1 (576.00k/576.00k flops)
  model/layer_normalization_1/batchnorm/mul (576.00k/576.00k flops)
  model/layer_normalization_1/batchnorm/mul_1 (576.00k/576.00k flops)
  model/layer_normalization_1/batchnorm/mul_2 (576.00k/576.00k flops)
  model/layer_normalization_1/batchnorm/sub (576.00k/576.00k flops)
  model/layer_normalization_1/moments/mean (576.00k/576.00k flops)
  model/layer_normalization_1/moments/variance (576.00k/576.00k flops)
  model/layer_normalization_2/batchnorm/add_1 (576.00k/576.00k flops)
  model/layer_normalization_2/batchnorm/mul (576.00k/576.00k flops)
  model/layer_normalization_2/batchnorm/mul_1 (576.00k/576.00k flops)
  model/layer_normalization_2/batchnorm/mul_2 (576.00k/576.00k flops)
  model/layer_normalization_2/batchnorm/sub (576.00k/576.00k flops)
  model/layer_normalization_2/moments/mean (576.00k/576.00k flops)
  model/layer_normalization_2/moments/variance (576.00k/576.00k flops)
  model/tf.__operators__.add_6/AddV2 (576.00k/576.00k flops)
  model/tf.__operators__.add_7/AddV2 (576.00k/576.00k flops)
  model/tf.__operators__.add_8/AddV2 (576.00k/576.00k flops)
  model/Gap0/AttnFeat/batch_normalization_1/FusedBatchNormV3 (360.01k/360.01k flops)
  model/Gap1/AttnFeat/batch_normalization_3/FusedBatchNormV3 (360.01k/360.01k flops)
  model/Gap0/AttnFeat/add (180.00k/180.00k flops)
  model/Gap0/AttnFeat/add_1 (180.00k/180.00k flops)
  model/Gap0/AttnFeat/add_2 (180.00k/180.00k flops)
  model/Gap0/AttnFeat/add_3 (180.00k/180.00k flops)
  model/Gap0/AttnFeat/conv2d_2/BiasAdd (180.00k/180.00k flops)
  model/Gap0/AttnFeat/mul_1 (180.00k/180.00k flops)
  model/Gap1/AttnFeat/add (180.00k/180.00k flops)
  model/Gap1/AttnFeat/add_1 (180.00k/180.00k flops)
  model/Gap1/AttnFeat/add_2 (180.00k/180.00k flops)
  model/Gap1/AttnFeat/add_3 (180.00k/180.00k flops)
  model/Gap1/AttnFeat/conv2d_6/BiasAdd (180.00k/180.00k flops)
  model/Gap1/AttnFeat/mul_1 (180.00k/180.00k flops)
  model/masking/NotEqual (144.00k/144.00k flops)
  model/masking/mul (144.00k/144.00k flops)
  model/Gap0/AttnFeat/batch_normalization/FusedBatchNormV3 (18.01k/18.01k flops)
  model/Gap1/AttnFeat/batch_normalization_2/FusedBatchNormV3 (18.01k/18.01k flops)
  model/layer_normalization/batchnorm/Rsqrt (18.00k/18.00k flops)
  model/layer_normalization_1/batchnorm/Rsqrt (18.00k/18.00k flops)
  model/layer_normalization_2/batchnorm/Rsqrt (18.00k/18.00k flops)
  model/tf.math.square/Square (18.00k/18.00k flops)
  model/Gap0/AttnFeat/conv2d_1/BiasAdd (9.00k/9.00k flops)
  model/Gap1/AttnFeat/conv2d_5/BiasAdd (9.00k/9.00k flops)
  model/dense_8/BiasAdd (9.00k/9.00k flops)
  model/layer_normalization/batchnorm/add (9.00k/9.00k flops)
  model/layer_normalization_1/batchnorm/add (9.00k/9.00k flops)
  model/layer_normalization_2/batchnorm/add (9.00k/9.00k flops)
  model/tf.__operators__.add_5/AddV2 (9.00k/9.00k flops)
  model/tf.__operators__.eq/Equal (9.00k/9.00k flops)
  model/tf.__operators__.ne/NotEqual (9.00k/9.00k flops)
  model/tf.math.greater/Greater (9.00k/9.00k flops)
  model/tf.math.multiply_1/Mul (9.00k/9.00k flops)
  model/tf.math.multiply_2/Mul (9.00k/9.00k flops)
  model/tf.math.negative_1/Neg (9.00k/9.00k flops)
  model/tf.math.reduce_sum/Sum (9.00k/9.00k flops)
  model/Gap0/AttnFeat/group_normalization/batchnorm/mul (64/64 flops)
  model/Gap0/AttnFeat/group_normalization/batchnorm/mul_2 (64/64 flops)
  model/Gap0/AttnFeat/group_normalization/batchnorm/sub (64/64 flops)
  model/Gap0/AttnFeat/group_normalization_1/batchnorm/mul (64/64 flops)
  model/Gap0/AttnFeat/group_normalization_1/batchnorm/mul_2 (64/64 flops)
  model/Gap0/AttnFeat/group_normalization_1/batchnorm/sub (64/64 flops)
  model/Gap1/AttnFeat/group_normalization_2/batchnorm/mul (64/64 flops)
  model/Gap1/AttnFeat/group_normalization_2/batchnorm/mul_2 (64/64 flops)
  model/Gap1/AttnFeat/group_normalization_2/batchnorm/sub (64/64 flops)
  model/Gap1/AttnFeat/group_normalization_3/batchnorm/mul (64/64 flops)
  model/Gap1/AttnFeat/group_normalization_3/batchnorm/mul_2 (64/64 flops)
  model/Gap1/AttnFeat/group_normalization_3/batchnorm/sub (64/64 flops)
  model/Gap0/AttnFeat/group_normalization/batchnorm/Rsqrt (8/8 flops)
  model/Gap0/AttnFeat/group_normalization_1/batchnorm/Rsqrt (8/8 flops)
  model/Gap1/AttnFeat/group_normalization_2/batchnorm/Rsqrt (8/8 flops)
  model/Gap1/AttnFeat/group_normalization_3/batchnorm/Rsqrt (8/8 flops)
  model/Gap0/AttnFeat/group_normalization/batchnorm/add (4/4 flops)
  model/Gap0/AttnFeat/group_normalization_1/batchnorm/add (4/4 flops)
  model/Gap1/AttnFeat/group_normalization_2/batchnorm/add (4/4 flops)
  model/Gap1/AttnFeat/group_normalization_3/batchnorm/add (4/4 flops)
  model/Gap0/AttnFeat/mul (1/1 flops)
  model/Gap0/AttnFeat/mul_2 (1/1 flops)
  model/Gap1/AttnFeat/mul (1/1 flops)
  model/Gap1/AttnFeat/mul_2 (1/1 flops)

======================End of Report==========================
flops 6589026844
flops 10**9 6.589026844
~270k trainable params

DISTILLNET; MLP

Model's state_dict:
l1.weight 	 torch.Size([128, 16])
l1.bias 	 torch.Size([128])
l2.weight 	 torch.Size([64, 128])
l2.bias 	 torch.Size([64])
bn2.weight 	 torch.Size([64])
bn2.bias 	 torch.Size([64])
bn2.running_mean 	 torch.Size([64])
bn2.running_var 	 torch.Size([64])
bn2.num_batches_tracked 	 torch.Size([])
l3.weight 	 torch.Size([1, 64])
l3.bias 	 torch.Size([1])


-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 10:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         10.62 K 
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       92.74 MMACs
fwd flops per GPU:                                                      188.35 M
fwd flops of model = fwd flops per GPU * mp_size:                       188.35 M
fwd latency:                                                            4.86 ms 
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    38.75 GFLOPS